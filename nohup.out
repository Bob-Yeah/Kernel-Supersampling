Namespace(act='relu', batch_size=16, beta1=0.9, beta2=0.999, chop=False, cpu=False, data_range='1-780/780-800', data_test='Set5', data_train='DIV2K', debug=False, decay_type='step', dilation=False, dir_data='/home/yejiannan/SIGGRAPH2022/data', dir_demo='../test', epochs=10, epsilon=1e-08, ext='img', extend='.', gamma=0.5, load='.', loss='1*L1', lr=0.0001, lr_decay=200, model='KernelSR', momentum=0.9, n_GPUs=1, n_colors=3, n_feats=64, n_resblocks=16, n_threads=6, no_augment=False, optimizer='ADAM', patch_size=50, pre_train='.', precision='single', print_every=50, res_scale=1, reset=False, resume=0, rgb_range=255, save='meta', save_models=True, save_results=True, scale='2', seed=1, self_ensemble=False, shift_mean=True, skip_threshold=1000000.0, split_batch=1, start_epoch=0, test_every=1000, test_only=False, weight_decay=0)
Preparing loss function:
1.000 * L1
train repeat:20
Set5
5
5
Making model...
RepVGG Block, identity =  None
RepVGG Block, identity =  BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
RepVGG Block, identity =  None
[Epoch 1]	Learning rate: 2.00e-4
/home/yejiannan/miniconda3/envs/PyTorch/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/yejiannan/miniconda3/envs/PyTorch/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/yejiannan/miniconda3/envs/PyTorch/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  "please use `get_last_lr()`.", UserWarning)
[800/15600]	[L1: 0.3687]	2.3+21.6s
[1600/15600]	[L1: 0.2620]	2.2+18.9s
[2400/15600]	[L1: 0.2121]	2.2+18.8s
[3200/15600]	[L1: 0.1827]	2.1+20.8s
[4000/15600]	[L1: 0.1618]	2.1+18.7s
[4800/15600]	[L1: 0.1472]	2.2+18.2s
[5600/15600]	[L1: 0.1361]	2.2+20.8s
[6400/15600]	[L1: 0.1275]	2.2+18.8s
[7200/15600]	[L1: 0.1205]	2.2+18.3s
[8000/15600]	[L1: 0.1147]	2.2+20.6s
[8800/15600]	[L1: 0.1096]	2.2+18.9s
[9600/15600]	[L1: 0.1052]	2.2+18.5s
[10400/15600]	[L1: 0.1012]	2.2+20.7s
[11200/15600]	[L1: 0.0977]	2.1+18.3s
[12000/15600]	[L1: 0.0946]	2.1+18.7s
[12800/15600]	[L1: 0.0919]	2.1+20.8s
[13600/15600]	[L1: 0.0894]	2.2+18.8s
[14400/15600]	[L1: 0.0871]	2.1+18.5s
[15200/15600]	[L1: 0.0849]	2.2+20.8s
[Epoch 2]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0453]	2.1+21.8s
[1600/15600]	[L1: 0.0453]	2.1+18.7s
[2400/15600]	[L1: 0.0450]	2.2+18.5s
[3200/15600]	[L1: 0.0448]	2.2+20.2s
[4000/15600]	[L1: 0.0443]	2.2+19.1s
[4800/15600]	[L1: 0.0440]	2.2+18.8s
[5600/15600]	[L1: 0.0436]	2.2+18.8s
[6400/15600]	[L1: 0.0435]	2.2+19.9s
[7200/15600]	[L1: 0.0431]	2.2+18.6s
[8000/15600]	[L1: 0.0429]	2.2+20.0s
[8800/15600]	[L1: 0.0427]	2.2+19.3s
[9600/15600]	[L1: 0.0424]	2.2+18.3s
[10400/15600]	[L1: 0.0423]	2.2+20.1s
[11200/15600]	[L1: 0.0420]	2.2+19.5s
[12000/15600]	[L1: 0.0419]	2.2+18.8s
[12800/15600]	[L1: 0.0417]	2.2+18.5s
[13600/15600]	[L1: 0.0414]	2.2+20.7s
[14400/15600]	[L1: 0.0411]	2.2+18.2s
[15200/15600]	[L1: 0.0411]	2.2+18.9s
[Epoch 3]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0375]	2.2+21.7s
[1600/15600]	[L1: 0.0366]	2.2+18.9s
[2400/15600]	[L1: 0.0364]	2.2+18.4s
[3200/15600]	[L1: 0.0367]	2.2+21.3s
[4000/15600]	[L1: 0.0365]	2.2+18.7s
[4800/15600]	[L1: 0.0360]	2.1+18.2s
[5600/15600]	[L1: 0.0359]	2.2+20.6s
[6400/15600]	[L1: 0.0357]	2.2+18.6s
[7200/15600]	[L1: 0.0355]	2.1+18.5s
[8000/15600]	[L1: 0.0355]	2.2+21.1s
[8800/15600]	[L1: 0.0354]	2.2+18.7s
[9600/15600]	[L1: 0.0353]	2.2+18.2s
[10400/15600]	[L1: 0.0353]	2.2+20.7s
[11200/15600]	[L1: 0.0351]	2.2+18.7s
[12000/15600]	[L1: 0.0350]	2.2+18.2s
[12800/15600]	[L1: 0.0349]	2.2+20.6s
[13600/15600]	[L1: 0.0349]	2.2+18.8s
[14400/15600]	[L1: 0.0349]	2.2+18.3s
[15200/15600]	[L1: 0.0348]	2.2+20.5s
[Epoch 4]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0327]	2.2+21.6s
[1600/15600]	[L1: 0.0327]	2.2+18.9s
[2400/15600]	[L1: 0.0328]	2.2+19.1s
[3200/15600]	[L1: 0.0328]	2.2+20.7s
[4000/15600]	[L1: 0.0325]	2.2+18.4s
[4800/15600]	[L1: 0.0326]	2.2+18.9s
[5600/15600]	[L1: 0.0327]	2.2+20.3s
[6400/15600]	[L1: 0.0326]	2.2+18.2s
[7200/15600]	[L1: 0.0326]	2.2+19.4s
[8000/15600]	[L1: 0.0324]	2.2+19.5s
[8800/15600]	[L1: 0.0324]	2.2+19.6s
[9600/15600]	[L1: 0.0323]	2.2+18.5s
[10400/15600]	[L1: 0.0322]	2.2+19.9s
[11200/15600]	[L1: 0.0323]	2.2+19.2s
[12000/15600]	[L1: 0.0322]	2.2+18.6s
[12800/15600]	[L1: 0.0321]	2.2+19.5s
[13600/15600]	[L1: 0.0320]	2.2+18.9s
[14400/15600]	[L1: 0.0319]	2.2+19.2s
[15200/15600]	[L1: 0.0319]	2.2+20.3s
[Epoch 5]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0323]	2.1+21.9s
[1600/15600]	[L1: 0.0314]	2.2+18.8s
[2400/15600]	[L1: 0.0309]	2.2+18.8s
[3200/15600]	[L1: 0.0309]	2.1+20.8s
[4000/15600]	[L1: 0.0310]	2.2+18.5s
[4800/15600]	[L1: 0.0310]	2.1+18.6s
[5600/15600]	[L1: 0.0310]	2.1+21.2s
[6400/15600]	[L1: 0.0308]	2.1+18.5s
[7200/15600]	[L1: 0.0308]	2.1+18.5s
[8000/15600]	[L1: 0.0307]	2.1+20.9s
[8800/15600]	[L1: 0.0306]	2.2+18.6s
[9600/15600]	[L1: 0.0305]	2.2+18.3s
[10400/15600]	[L1: 0.0306]	2.2+20.7s
[11200/15600]	[L1: 0.0306]	2.2+18.7s
[12000/15600]	[L1: 0.0305]	2.2+18.1s
[12800/15600]	[L1: 0.0305]	2.2+20.8s
[13600/15600]	[L1: 0.0304]	2.2+18.5s
[14400/15600]	[L1: 0.0304]	2.1+18.2s
[15200/15600]	[L1: 0.0303]	2.2+20.8s
[Epoch 6]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0295]	2.3+21.4s
[1600/15600]	[L1: 0.0298]	2.2+18.8s
[2400/15600]	[L1: 0.0298]	2.2+18.5s
[3200/15600]	[L1: 0.0295]	2.2+20.0s
[4000/15600]	[L1: 0.0295]	2.2+19.2s
[4800/15600]	[L1: 0.0293]	2.2+18.3s
[5600/15600]	[L1: 0.0294]	2.2+20.3s
[6400/15600]	[L1: 0.0294]	2.2+19.7s
[7200/15600]	[L1: 0.0293]	2.2+18.6s
[8000/15600]	[L1: 0.0293]	2.2+19.2s
[8800/15600]	[L1: 0.0292]	2.2+20.7s
[9600/15600]	[L1: 0.0291]	2.1+19.3s
[10400/15600]	[L1: 0.0290]	2.2+18.8s
[11200/15600]	[L1: 0.0290]	2.1+20.6s
[12000/15600]	[L1: 0.0290]	2.2+18.3s
[12800/15600]	[L1: 0.0289]	2.2+18.3s
[13600/15600]	[L1: 0.0289]	2.2+20.6s
[14400/15600]	[L1: 0.0288]	2.2+18.4s
[15200/15600]	[L1: 0.0287]	2.2+18.3s
[Epoch 7]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0275]	2.2+21.9s
[1600/15600]	[L1: 0.0287]	2.2+18.9s
[2400/15600]	[L1: 0.0283]	2.2+18.7s
[3200/15600]	[L1: 0.0284]	2.2+21.0s
[4000/15600]	[L1: 0.0284]	2.2+18.1s
[4800/15600]	[L1: 0.0282]	2.2+18.3s
[5600/15600]	[L1: 0.0281]	2.2+20.4s
[6400/15600]	[L1: 0.0282]	2.2+19.0s
[7200/15600]	[L1: 0.0280]	2.2+18.7s
[8000/15600]	[L1: 0.0278]	2.2+19.3s
[8800/15600]	[L1: 0.0279]	2.2+20.0s
[9600/15600]	[L1: 0.0279]	2.2+18.4s
[10400/15600]	[L1: 0.0280]	2.2+20.1s
[11200/15600]	[L1: 0.0278]	2.2+19.3s
[12000/15600]	[L1: 0.0278]	2.2+18.0s
[12800/15600]	[L1: 0.0278]	2.2+20.4s
[13600/15600]	[L1: 0.0278]	2.2+18.5s
[14400/15600]	[L1: 0.0277]	2.2+18.8s
[15200/15600]	[L1: 0.0278]	2.2+19.9s
[Epoch 8]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0254]	2.2+21.9s
[1600/15600]	[L1: 0.0257]	2.2+19.2s
[2400/15600]	[L1: 0.0261]	2.2+18.5s
[3200/15600]	[L1: 0.0263]	2.2+20.6s
[4000/15600]	[L1: 0.0264]	2.2+19.0s
[4800/15600]	[L1: 0.0264]	2.2+18.7s
[5600/15600]	[L1: 0.0265]	2.2+19.8s
[6400/15600]	[L1: 0.0267]	2.2+20.1s
[7200/15600]	[L1: 0.0267]	2.2+18.6s
[8000/15600]	[L1: 0.0268]	2.2+18.9s
[8800/15600]	[L1: 0.0267]	2.2+20.1s
[9600/15600]	[L1: 0.0265]	2.2+18.5s
[10400/15600]	[L1: 0.0266]	2.2+18.8s
[11200/15600]	[L1: 0.0265]	2.2+20.3s
[12000/15600]	[L1: 0.0264]	2.2+18.2s
[12800/15600]	[L1: 0.0264]	2.2+19.9s
[13600/15600]	[L1: 0.0264]	2.2+19.0s
[14400/15600]	[L1: 0.0264]	2.2+18.4s
[15200/15600]	[L1: 0.0264]	2.2+21.1s
[Epoch 9]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0257]	2.2+21.5s
[1600/15600]	[L1: 0.0260]	2.2+18.9s
[2400/15600]	[L1: 0.0260]	2.2+18.8s
[3200/15600]	[L1: 0.0261]	2.2+20.0s
[4000/15600]	[L1: 0.0259]	2.2+19.1s
[4800/15600]	[L1: 0.0259]	2.2+18.7s
[5600/15600]	[L1: 0.0259]	2.2+19.9s
[6400/15600]	[L1: 0.0257]	2.2+19.2s
[7200/15600]	[L1: 0.0256]	2.2+18.7s
[8000/15600]	[L1: 0.0257]	2.2+20.3s
[8800/15600]	[L1: 0.0257]	2.2+18.6s
[9600/15600]	[L1: 0.0256]	2.2+18.2s
[10400/15600]	[L1: 0.0256]	2.2+20.7s
[11200/15600]	[L1: 0.0256]	2.2+19.1s
[12000/15600]	[L1: 0.0256]	2.2+18.4s
[12800/15600]	[L1: 0.0255]	2.2+19.0s
[13600/15600]	[L1: 0.0255]	2.2+20.4s
[14400/15600]	[L1: 0.0255]	2.2+18.4s
[15200/15600]	[L1: 0.0255]	2.2+19.1s
[Epoch 10]	Learning rate: 2.00e-4
[800/15600]	[L1: 0.0258]	2.2+21.9s
[1600/15600]	[L1: 0.0259]	2.2+18.2s
[2400/15600]	[L1: 0.0256]	2.2+18.6s
[3200/15600]	[L1: 0.0254]	2.2+20.9s
[4000/15600]	[L1: 0.0254]	2.2+18.7s
[4800/15600]	[L1: 0.0253]	2.2+18.7s
[5600/15600]	[L1: 0.0253]	2.2+21.3s
[6400/15600]	[L1: 0.0253]	2.2+18.5s
[7200/15600]	[L1: 0.0254]	2.2+18.3s
[8000/15600]	[L1: 0.0253]	2.2+20.9s
[8800/15600]	[L1: 0.0254]	2.2+18.6s
[9600/15600]	[L1: 0.0253]	2.2+18.5s
[10400/15600]	[L1: 0.0252]	2.2+20.8s
[11200/15600]	[L1: 0.0252]	2.2+18.3s
[12000/15600]	[L1: 0.0252]	2.2+18.4s
[12800/15600]	[L1: 0.0252]	2.2+20.9s
[13600/15600]	[L1: 0.0251]	2.2+18.2s
[14400/15600]	[L1: 0.0251]	2.2+18.4s
[15200/15600]	[L1: 0.0251]	2.2+20.9s
